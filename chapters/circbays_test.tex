\documentclass{article}
\usepackage[utf8]{inputenc}

\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
	Kees Mulder\\Utrecht University \And Jolien Cremers\\Utrecht University \And Irene Klugkist\\Utrecht University
}
\title{\pkg{circbayes}: An \proglang{R} Package for Bayesian Circular
	Statistics}


\Keywords{circular statistics, mcmc, von mises, projected normal, circular regression, bayes factor, dirichlet process}
\Plainkeywords{circular statistics, mcmc, von mises, projected normal, circular regression, bayes factor, dirichlet process}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
	Kees Mulder\\
	Utrecht University\\
	Padualaan 14 Utrecht\\
	E-mail: \email{k.t.mulder@uu.nl}\\
	
	Jolien Cremers\\
	Utrecht University\\
	Padualaan 14 Utrecht\\
	E-mail: \email{j.cremers@uu.nl}\\
	
	Irene Klugkist\\
	Utrecht University\\
	Padualaan 14 Utrecht\\
	E-mail: \email{i.klugkist@uu.nl}\\
	
}

% Pandoc header

\usepackage{amsmath} \usepackage{amsfonts} \usepackage{subcaption}
\usepackage{amssymb}

\newcommand{\btheta}{\boldsymbol{\theta}} \newcommand{\sumin}{\sum_{i=1}^n} \newcommand{\wavg}{\frac{1}{n} \sum_{i=1}^n} \newcommand{\kp}{\kappa}

\DeclareMathOperator{\atantwo}{atan2}

\newcommand{\bx}{\boldsymbol{x}} \newcommand{\bX}{\boldsymbol{X}} \newcommand{\bt}{\boldsymbol{\theta}} \newcommand{\bd}{\boldsymbol{d}} \newcommand{\bdt}{\boldsymbol{\delta}} \newcommand{\bbt}{\boldsymbol{\beta}} \newcommand{\bps}{\boldsymbol{\psi}} \newcommand{\bph}{\boldsymbol{\phi}} \newcommand{\bmu}{\boldsymbol{\mu}} \newcommand{\bkp}{\boldsymbol{\kappa}} \newcommand{\balph}{\boldsymbol{\alpha}} \newcommand{\blam}{\boldsymbol{\lambda}} \newcommand{\bgam}{\boldsymbol{\gamma}} \newcommand{\thedata}{\boldsymbol{\theta}, \boldsymbol{X}, \boldsymbol{d}} \newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

\begin{document}


Circular data consist of quantities defined on the unit circle, such
that the sample space is periodic. This type of data can be generated by
a number of natural mechanisms or measurement instruments. Wind
directions, for example, can directly be observed as angles. Time can be
viewed as periodic if one is interested in the time of day, and
similarly day of the week, month or year. A polar transformation on
bivariate data results in linear and a directional component, which may
represent the processes of interest better than the original bivariate
data. Common between these is that the resulting data can be represented
as angles in radians or degrees, and that they require specialized
statistical methods, from a branch of statistics called circular
statistics.

To see this, one might consider two angles \(10^\circ\) and
\(350^\circ.\) Treating these as numbers on the real line results in a
difference of \(340^\circ\) and a arithmetic mean of \(180^\circ\),
which do not correspond with an intuitive difference and mean. Instead,
their difference on the circle (the arc length between them) is
\(20^\circ\), and their mean direction is \(0^\circ\).

Circular data are encountered in a wide variety of scientific fields,
including biology \citep{nunez2018bayesian}, aerospace
\citep{kurz2017deterministic}, political sciences \citep{gill2010},
machine learning \citep{gopal2014mises}, signal processing
\citep{traa2013wrapped}, life sciences \citep{mardianew}, motor
behaviour research
\citep{mechsner2001perceptual, mechsner2007bimanual, postma2008keep, baayen2012test},
behavioural biology \citep{bulbert2015danger}, psychology
\citep{Leary1957, gurtman2003circumplex, kaas2006haptic,  gurtman2009exploring},
bioinformatics \citep{mardia2008multivariate} and environmental sciences
\citep{lagona2016regression, lagona2015hidden, arnold2006recent}.

Statistical models for circular data have been developed in the field of
circular statistics
\citep{fisher1995statistical, jammalamadaka2001topics, mardia2009directional, pewsey2013circular, ley2017modern, ley2018applied}.
Besides redefining the difference, mean, and other summary statistics,
circular statistics relies on probability distributions defined on the
circle, such as the von Mises distribution \citep{von1918ganzzahligkeit}
or Projected Normal distribution \citep{Kendall1974} (which is also
known as the Displaced or Offset Normal distribution).

Several packages to deal with circular data exist (and will be recapped
in Section \ref{relatedwork}), but no packages are available for
Bayesian inference of circular data. However, there are several major
advantages of the Bayesian approach, some of which are more general,
while others are specific to the field of circular data. General
advantages of the Bayesian approach are, among others, the ability to
include prior information if desired, while objective approaches are
also possible \citep{berger2006case}, the lack of asymptotic assumptions
(such that analyses are also valid for small samples), flexibility in
developing models, and an appealing hypothesis testing toolkit in the
vein of \citet{jeffreys1961theory}.

Some other properties of the Bayesian approach approach are particularly
attractive when dealing with circular data. First, in the Bayesian
approach it is trivially easy to obtain additional inference on
functions of the parameters of a model. This is beneficial if the model
form does not directly include interpretable parameters. This is
exploited for the regression parameters of the Projected Normal
regression model (Section \ref{projreg}) and the circular variance
component of the Inverse Batschelet distribution (Section \ref{mixmod}).
Second, some models such as the Projected Normal regression models use
data augmentation facilitate inference algorithms, which is relatively
simple in the Bayesian approach. Third, some likelihoods encountered in
circular statistics are somewhat ill-behaved, such as those of the
regression parameters of the von Mises regression model. This can be
solved by introducing a specific prior on these parameters. Fourth, some
circular data models are of such mathematical form that deriving
standard errors is difficult, while the Bayesian approach allows us to
compute fully our uncertainty around some estimated parameters by
investigating the posterior distribution.

To facilitate the use of Bayesian circular statistics, in this paper we
introduce the \proglang{R} package \pkg{circbayes}, which is a diverse
collection of methods for Bayesian inference on circular data. These
methods include density estimation, regression models, hypothesis tests,
mixture models and non-parametric models. Inference is generally
performed through custom Markov chain Monte Carlo (MCMC) algorithms.

The rest of this paper will be structured as follows. Section
\ref{relatedwork} summarizes related packages for circular data
analysis. Some general remarks on the structure of the package are given
in Section \ref{sec:pkgstruct}. Basics of circular statistics are
recapped in Section \ref{circstats}. Section \ref{circreg} provides
Bayesian inference for two types of circular regression models, one von
Mises based and the other Projected Normal based. Density estimation
problems are addressed in Section \ref{densest}, using mixture models,
both parametric in form as well as using a Dirichlet Process prior.
Section \ref{hyptest} provides several Bayesian hypothesis tests,
including examples for testing circular uniformity as well as tests for
group differences (such as ANOVA). Finally, some concluding remarks are
given in Section \ref{discussion}.

\hypertarget{related-work}{%
\section{Related work}\label{related-work}}

\label{relatedwork}

Three different groups of software packages related to circular
statistics can be identified.

The first group are general packages for circular statistics including
functions to obtain summary statistics, plots, hypothesis tests, and
regression models. General packages for circular statistics include the
\proglang{Stata} toolbox \pkg{CIRCSTAT} \citep{cox1998circstat}, the
\proglang{MATLAB} toolbox \pkg{CircStats} \citep{berens2009circstat},
and the \proglang{R} package \pkg{circular} \citep{lund2013package}. The
package \pkg{circbayes} falls in this first category, providing less
specialized functionality, but it is unique in the fact that it provides
Bayesian inference for a variety of standard problems in circular data.

Second, specific problems may require a single solution from the field
of circular statistics, which are then packaged and often published in
domain-specific journals. For example, in behavioural biology animal
movements can be modeled using hidden Markov Models with circular
regression components, which are implemented in \pkg{momentuHMM}
\citep{mcclintock2018momentuhmm}. In criminology, spatial analysis with
an interval-censored crime time can be performed using the package
\pkg{aoristic} \citep{kikuchi2015package}. The package \pkg{NPCirc}
\citep{JSSv061i09} contains a variety of methods for non-parametric
models on the circle. In cell biology, the cell division cycle calls for
isotonic circular regression models, provided in \pkg{isocir}
\citep{barragan2013isocir}. The \proglang{R} package \pkg{circE}
\citep{grassi2010circe} implements circumplex models, which represent a
type of latent factor analysis where the items can reasonably be
constrained on a unit circle \citep{browne1992circumplex}. Methods for
analysis of circular data by wrapping distributions on the real line
around the circle are provided in the \proglang{R} package \pkg{wrapped}
\citep{nadarajah2017wrapped}. Directional distributions for use within
dynamic Bayesian networks are provided by \pkg{Mocapy++}
\citep{paluszewski2010mocapy}, which has useful applications in
bioinformatics.

The third group are packages made for directional data, defined on
\(p\)-dimensional hyperspheres. Because the circle is a 1-dimensional
hypersphere, these packages often also provide functionality for
circular data. An early package in \proglang{MATLAB} is \pkg{SPAK}
\citep{leong1998methods}, providing spherical data analysis and
visualization. Von Mises-Fisher Mixture models in any dimension can be
fit in \proglang{R} using \pkg{movMF} \citep{hornik2014movmf}. The
\proglang{MATLAB} package \pkg{libDirectional}
\citep{kurz2017directional} implements methods for several sophisticated
directional probability distributions as well as filtering. Methods
specific to operations on objects oriented in three-dimensional space
are available in the \proglang{R} packages \pkg{orientlib}
\citep{murdoch2003orientlib} and \pkg{rotations}
\citep{stanfill2014rotations}.

An alternative approach to perform Bayesian analyses of circular data is
to use one of the probabilistic programming frameworks popular in
Bayesian statistics, such as \proglang{JAGS}, \proglang{BUGS} or
\proglang{Stan}. An issue is that these generally lack support for
circular observations and circular parameters, although this may be
circumvented by representing these as bivariate vectors constrained to
lie on the unit circle. Some rudimentary support for circular
probability distributions is available in some of these frameworks,
usually for the von Mises distribution only. In \proglang{JAGS}, this
requires installing an extension \citep{jagsvonmises}.

The main problem in working with circular data in these probabilistic
programming frameworks is that both the MCMC algorithm as well as
methods for analysis of the MCMC output generally do not take into
account the circular nature of the parameters. For example, to summarise
a credible interval of a circular mean parameter, we must use circular
credible intervals, which none of the probabilistic programming
frameworks provides. An additional issue is that the MCMC algorithms
implemented in \pkg{circbayes} often use custom steps in order to speed
up the algorithm in some way, many of which are impossible or
time-consuming to implement in these frameworks. Finally, plotting must
also be reconsidered for circular parameters.

\hypertarget{general-package-structure}{%
\section{General package structure}\label{general-package-structure}}

\label{sec:pkgstruct}

The package \pkg{circbayes} is a collection of Bayesian inference
algorithms for statistical models of circular observations, along with
methods to analyze and compare the resulting fits. Each model is called
through a fit function. Table \ref{tab:models} provides an overview of
the names of the available methods as well as some of their properties,
which will be described in more detail in the rest of this paper.

While programming languages usually work with angles described by
radians, some users prefer to use other units such as degrees or hours.
In this case, the user is recommended to work with their circular data
as a \code{circular} S3 object from the package \pkg{circular}. Care
must be taken that the \code{circular} object is given the correct unit
type of the data, by setting the \code{units} argument accordingly.

\begin{CodeChunk}

\begin{CodeInput}
R> th_numeric  <- c(100, 180, 340)
R> th_circular <- circular::circular(th_numeric, units = "degrees")
\end{CodeInput}
\end{CodeChunk}

Then, if these are entered into the fit functions of \pkg{circbayes},
they are automatically transformed to radians for internal use.

Because inference is generally done through posterior simulation (such
as MCMC), several methods are always available. Results from the MCMC
algorithms are returned as S3 objects (named by appending \code{_mod} to
the function name), which provide easy access to these methods. In
particular, methods such as \code{posterior_samples()}, \code{plot()},
\code{print()}, \code{coef()}, \code{marg_lik()} and \code{inf_crit()}
are available for all models. Throughout all outputs, the circular
paramaters, such as mean direction parameters (ie. \(\mu\)) are treated
as such, so `mean' or `median' refers to the circular mean and median,
for example. Lower and upper bounds given are generally highest
posterior density (HPD) intervals. Modes are estimated as the midpoint
of a short (ie. \(10\%\)) HPD interval \citep{venter1967estimation}.

The \code{plot()} methods for each model follow a similar pattern.
Univariate models can be plot either in polar coordinates or on the
cartesian plane, along with the best fit for the probability density
function. Regression models can be plot as bivariate plots where a
predictor can be selected to place on the x-axis. In addition, we
exploit the fact each iteration of an MCMC implies either a probability
density function (given the current parameters) or a regression function
(given the current parameters). As a result, we can plot the uncertainty
as a number of samples from the probability density or regression
functions from the posterior, using the option \code{n_samples}. Also,
to reduce visual noise, the uncertainty can be plotted as credible
intervals of these functions, using the options \code{add_ci}.

Typical options for MCMC methods are available throughout. Setting
\code{niter} allows the user to set the number of iterations to sample,
while \code{burnin} allows the user to set the number of initial
iterations to burn should the sampler take some time to converge, and
finally \code{thin} allows the user to set a thinning factor, which will
only keep a fraction of \code{1/thin} of the posterior samples, which
can be useful to save on memory.

\begin{table}[t]
\small
\centering
\begin{tabular}{lrrrr}
Model & Fit function & MCMC & Marginal Likelihood & Nested \\ \hline
Circular uniform & - & - & \((2\pi)^{-n}\) & - \\
Von Mises & \code{vm_posterior} & Yes & \(\int_0^\infty I_0(R_n \kappa) / I_0(\kappa)^m d\kappa\) & -\\
Batschelet & \code{bat_posterior} & Yes & Bridgesampling & -\\
Von Mises Regression & \code{vm_reg} & Yes & Bridgesampling & Yes\\
Projected Normal & \code{pn_posterior} & Yes & Bridgesampling & -\\
Projected Normal Regression & \code{pn_reg} & Yes & Bridgesampling & Yes\\
Hierarchical PN Regression & \code{pn_me_reg} & Yes & Bridgesampling & Yes\\
VM Mixture Models & \code{vm_mix}& Yes & Bridgesampling & -\\
Batschelet Mixture Models & \code{bat_mix} & Yes & Bridgesampling & -\\
Dirichlet Process Models & \code{vm_dpm} & Yes & - & - \\
\end{tabular}
\caption{Overview of methods available for each model in \pkg{circbayes}. The columns represent the name of the model, the function to fit the model, whether the model is sampled with MCMC, how the marginal likelihood is calculated (see Section \ref{hyptest}), and whether model comparison through nested submodels is possible.} \label{tab:models}
\end{table}

\hypertarget{circular-statistics}{%
\section{Circular statistics}\label{circular-statistics}}

\label{circstats}

Circular data requires descriptive statistics and probability
distributions that are distinct from the usual linear statistics.
Because these will be used throughout, they will shortly be recapped
here. However, an extensive set of descriptive statistics is available
in \pkg{circular}, so functions for these are not repeated in
\pkg{circbayes}.

\hypertarget{summary-statistics}{%
\subsection{Summary statistics}\label{summary-statistics}}

Let \(\boldsymbol{\theta}= \theta_1, \dots, \theta_n\) be a set of
circular observations, where \(\theta_i \in [-\pi, \pi)\) . The
resultant length \(R\in[0, n]\) is a measure of spread and depends on
the sum of cosines and the sum of sines of the circular observations, so
that \begin{align}
C &= \sum_{i=1}^n\cos \theta_i, \qquad
S = \sum_{i=1}^n\sin \theta_i, \qquad
R = \sqrt{C^2 + S^2}.
\end{align} It is also useful to compute the mean resultant length
\(\bar{R} = R/n,\) which is useful because \(\bar{R} \in [0, 1],\) so it
is comparable between datasets of different sizes.

The most important central tendency for circular data, the mean
direction \(\bar\theta\), can then be defined as \begin{align}
\cos \bar \theta = C/R, \qquad
\sin \bar \theta = S/R,
\end{align} which computationally can be obtained through
\(\bar \theta = \text{atan2}(S, C),\) where \(\text{atan2}\) is the
two-argument arctangent function, which computes the direction from the
origin to the point \((C, S)\). This function is directly implemented in
\proglang{R} as \code{atan2}.

\hypertarget{probability-distributions}{%
\subsection{Probability distributions}\label{probability-distributions}}

\label{sec:probdist} Two approaches to derive circular distributions are
used in \pkg{circbayes}. First, the intrinsic approach is described in
Section \ref{sec:intrinsic}. Second, the embedding approach is described
in Section \ref{sec:embed}. Throughout, the models will be applied to
the wind data \citep{agostinelli2007robust, fisher1995statistical},
which is available in \pkg{circular} as \code{circular::wind}.

\hypertarget{intrinsic-approach}{%
\subsubsection{Intrinsic approach}\label{intrinsic-approach}}

\label{sec:intrinsic}

The intrinsic approach directly defines probability distributions on the
circle. We provide methods for the von Mises distribution and an
extension which allows for peaked and flat-topped densities, called
Batschelet distributions.

\hypertarget{von-mises-distribution}{%
\paragraph{Von Mises distribution}\label{von-mises-distribution}}

\label{vonmisespost}

The von Mises distribution is a symmetric and unimodal distribution on
the circle, given by \begin{equation}
\mathcal{M}(\theta \mid \mu, \kappa) = [2 \pi I_0(\kappa)]^{-1} \exp \left\{ \kappa \cos(\theta - \mu) \right\},
\end{equation} where \(\mu\) is the mean direction parameter, \(\kappa\)
is a concentration parameter, and \(I_0(\cdot)\) is the modified Bessel
function of the first kind and order zero. While of simple form, the
Bessel function in the normalizing constant makes direct inference for
\(\{\mu, \kappa\}\) difficult.

To facilitate computation, we use conjugate priors where possible. A
conjugate prior for \((\mu, \kappa)\) is given by
\citet{guttorp1988finding} as \begin{equation} \label{eqn:vmconjprior}
p(\mu, \kappa) \propto [I_0(\kappa)]^{-c}  \exp \left\{ R_0 \kappa \cos(\mu - \mu_0) \right\},
\end{equation} where \(\mu_0\) is the prior mean direction, \(R_0\) is
the prior resultant length, and \(c\) can be interpreted as a prior
`sample size'. Then, the von Mises posterior is given by
\begin{equation}
p(\mu, \kappa \mid \boldsymbol{\theta}) \propto [I_0(\kappa)]^{-m}  \exp \left\{ R_n \kappa \cos(\mu - \mu_n) \right\},
\end{equation} where the posterior summary statistics
\((\mu_n, R_n, m)\) are computed by first obtaining \(C_n = C + C_0,\)
\(S_n = S + S_0,\) then computing \(R_n = \sqrt{C_n^2 + S_n^2},\)
\(\mu_n = \text{atan2}(S_n, C_n),\) and \(m = n + c\).

Bayesian inference for \(\mu\) and \(\kappa\) can proceed through Gibbs
sampling \citep{chib1995understanding}. The full conditional
distribution of \(\mu\) is von Mises, and given by \begin{equation}
p(\mu \mid \kappa, \boldsymbol{\theta}) = \mathcal{M}(\mu_n, R_n \kappa)
\end{equation} from which efficient algorithm are available from
\citet{best1981bias}. While that algorithm for random sampling from the
von Mises distribution is available in \pkg{circular}, for efficiency a
version was implemented in \proglang{C++} through \pkg{Rcpp}
\citep{rcpp}, and provided in \pkg{circbayes} as \code{rvm}.

\begin{CodeChunk}

\begin{CodeInput}
R> th_ran <- rvm(100, mu = 2, kp = 10)
\end{CodeInput}
\end{CodeChunk}

The conditional distribution of \(\kappa\) is not of closed form. The
most efficient algorithm to sample \(\kappa\) is given by
\citet{forbes2015fast}, so we have implemented their rejection sampler
in \proglang{C++} as well.

An MCMC sample from the von Mises posterior with an uninformative prior
can be obtained by setting the option the prior parameters to
\(\mu_0 = 0, R_0 = 0, c = 0,\) or an informative prior can be selected.

\begin{CodeChunk}

\begin{CodeInput}
R> th     <- circular::wind
R> vm_mod <- vm_posterior(th, prior = c(mu_0 = 0, R_0 = 0, c = 0))
R> vm_mod
\end{CodeInput}

\begin{CodeOutput}
      estimate    se  2.5% 97.5%
mu       0.290 0.054 0.182 0.398
kappa    1.766 0.151 1.460 2.021
\end{CodeOutput}
\end{CodeChunk}

\hypertarget{batschelet-distribution}{%
\paragraph{Batschelet Distribution}\label{batschelet-distribution}}

The family of densities introduced by \citet{batschelet1981circular}
allows for peaked or flat-topped distributions on the circle. They are
constructed by transforming the input of the probability denstity
\(\theta\) through a function \(t_{\lambda}(\theta).\) This value is
then passed into some base density \(f_0\) of choice
\citep{abe2010symmetric, pewsey2011extension}. Then, the Batschelet
distribution is given by \(f(\theta) \propto f_0(t_\lambda(\theta)),\)
where \(\lambda \in (-1, 1)\) determines the shape of the distribution,
usually with \(\lambda = 0\) meaning no transformation. We implement two
choices of \(t_{\lambda}(\theta),\) but we always take the von Mises
distribution as the base density.

The von Mises based symmetric Inverse Batschelet (IB) density as given
in \citet{jones2012inverse} can be written as
\begin{equation} \label{eqn:invbatpdf}
 f_{IB}(\theta \mid \mu, \kappa, \lambda) = [2\pi I_0(\kappa)K_{\kappa, \lambda}]^{-1} \exp\{\kappa \cos t_\lambda(\theta - \mu)\}
\end{equation} where \begin{equation} \label{eqn:invbattransform}
  t_\lambda(\theta) = \frac{1 - \lambda}{1 + \lambda}\theta + \frac{2\lambda}{1 + \lambda} s_\lambda^{-1}(\theta),
\end{equation} with \(s_\lambda^{-1}(\theta)\) is the inverse of
\(s_\lambda(\theta) = \theta - \frac{1}{2} (1 + \lambda) \sin(\theta),\)
and the normalizing constant given by \begin{equation}
   K_{\kappa, \lambda} = \frac{1 + \lambda}{1 - \lambda} - \frac{2\lambda}{1 - \lambda} \int_{-\pi}^\pi [2\pi I_0(\kappa)]^{-1} \exp\left\{\kappa \cos \left(\theta -  (1 - \lambda) \sin(\theta) / 2 \right) \right\} d\theta.
\end{equation} Because of the implicit inverse in the transformation
function, this probability density is relatively costly to compute,
requiring a numerical inverse.

Therefore, an alternative transformation function is also available
which gives the distribution we call the Power Batschelet distribution.
It is constructed to approximate shape of the IB distribution, while
being easier to compute. The transformation function used for the Power
Batschelet distribution is given by \begin{equation}
  t_{\lambda}^\ast(\theta) = \text{sign}(\theta)\pi \left( \frac{\vert\theta\vert}{\pi} \right)^{\gamma(\lambda)},
\end{equation} where
\(\gamma(\lambda) = \frac{1 - c\lambda}{1 + c\lambda},\) where \(c\) is
some fixed constant, set to \(c = 0.4052284\) to approximate the IB
density closely. The normalizing constant is changed accordingly.

Regardless of the chosen option, there is no longer a conjugate prior
that can be used. Therefore, any non-conjugate prior can be selected.
The priors are assumed to be independent, however, and can be provided
as functions that return log-probability of the prior (up to an additive
constant). For example, we can obtain a sample from the posterior of the
inverse Batschelet distribution with a circular uniform prior on \(\mu\)
(the default), a \(\text{Gamma(2, 0.1)}\) prior on \(\kappa,\) and a
\(Beta(3, 3)\) prior on \((\lambda + 1)/2\) (where the shifting is done
to place the prior on the range of \(\lambda\)), as follows.

\begin{CodeChunk}

\begin{CodeInput}
R> kp_logp  <- function(kp) dgamma(kp, 2, 1/10, log = TRUE)
R> lam_logp <- function(lam) dbeta((lam + 1)/2, 3, 3, log = TRUE)
R> 
R> pb_mod <- bat_posterior(th, 
R+                         kp_logprior_fun  = kp_logp,
R+                         lam_logprior_fun = lam_logp)
R> pb_mod
\end{CodeInput}

\begin{CodeOutput}
               mean median    se  2.5% 97.5%
mu            0.157  0.149 0.023 0.143 0.216
kp            1.757  1.719 0.175 1.616 1.934
lam           0.830  0.856 0.145 0.268 0.959
circ_variance 0.334  0.338 0.028 0.283 0.375
circ_sd       0.901  0.909 0.047 0.815 0.969
\end{CodeOutput}
\end{CodeChunk}

This distribution does not have interpretable parameters beyond \(\mu,\)
because \(\kappa\) and \(\lambda\) both determine variance and shape.
Therefore, results are also provided for derived quantities in the
circular variance and circular standard deviation, as given in
\citep{fisher1995statistical}. The Power Batschelet can be selected by
setting \code{bat_type = "power"}, while the Inverse Batschelet is
obtained by \code{bat_type = "inverse"}.

\hypertarget{embedding-approach}{%
\subsubsection{Embedding approach}\label{embedding-approach}}

\label{sec:embed}

The embedding (or projected) approach treats the circular observations
by seeing them as projected bivariate observations. The idea behind this
projection is that we do not have to conduct inference on the circular
variable \(\Theta \in [-\pi, \pi)\) directly, but we can indirectly
conduct inference on a bivariate variable \(Y \in \mathbb{R}^2\). The
bivariate variable \(Y\) is not observed but inferred from \(\Theta\).
Their relation is given by
\begin{equation} \boldsymbol{y} = \boldsymbol{u}r, \end{equation} where
\(\boldsymbol{u} = (\cos \theta, \sin \theta)\) and
\(r = \vert\vert \boldsymbol{y} \vert\vert\). A common example of a
projected distribution is the Projected Normal (PN) distribution, given
by \begin{equation}
PN(\theta \mid \boldsymbol{\mu}, \boldsymbol{I})  = \frac{1}{2 \pi} e^{-\frac{1}{2}\vert \vert \boldsymbol\mu \vert \vert ^ 2} \left[1+\frac{\boldsymbol{u}^t\boldsymbol\mu\Phi(\boldsymbol{u}^t\boldsymbol\mu)}{\phi(\boldsymbol{u}^t\boldsymbol\mu)}\right],
\label{eq:PNdistribution}
\end{equation} where \(\theta \in [-\pi, \pi)\) is the circular
observation,
\(\boldsymbol{\mu} = (\mu_{1}, \mu_{2})^{t} \in \mathbb{R}^2\),
\(\boldsymbol{I}\) is the identity matrix,
\(\boldsymbol{u} = (\cos \theta, \sin \theta)^{t}\) and \(\Phi(\cdot)\)
and \(\phi(\cdot)\) denote the cumulative distribution function and
probability density function of the standard normal distribution,
respectively. The Projected Normal distribution is rotationally
symmetric about its mean direction
\(\boldsymbol{\mu}/\vert\vert\boldsymbol{\mu}\vert\vert\) and its
concentration is dependent on \(\vert\vert\boldsymbol{\mu}\vert\vert^2\)
(see \citet{Kendall1974} for the exact form of this relation). Note that
the covariance matrix is specified to be identity for identification
purposes. An alternative parametrization of the Projected Normal
distribution can be found in \citet{wang2012directional}.

A conjugate prior for \(\boldsymbol{\mu}\) is a bivariate Normal
distribution \(N_2(\boldsymbol{\mu}_0, \lambda_0\boldsymbol{I}),\)
resulting in a bivariate Normal conditional posterior
\citep{Nunez-Antonio2005} \begin{equation}
p(\boldsymbol{\mu} \mid \boldsymbol{X}) = N_2(\boldsymbol{\mu}_n, \boldsymbol{\Lambda}_n),
\end{equation} where \(\boldsymbol{X}\) is a \(n\times2\) design matrix,
\(\boldsymbol{\mu}_n\) is a two dimensional vector where each dimension
\(j \in {I,II}\) is defined by
\((\lambda_0\mu_0^j + n\bar{\boldsymbol{X}}_{j})/(\lambda_0 + n)\) and
\(n\) equals the sample size. The \(2\times2\) matrix
\(\boldsymbol{\Lambda}_n\) is diagonal with j\(^th\) element given by
\(\lambda_0 + n\). The conditional density for \(r\) is given by
\begin{equation}
p(r \mid \boldsymbol{\theta}, \boldsymbol{\mu}) \propto r \exp\{-0.5r^2 + br\},
\end{equation} where \(b = \boldsymbol{u}^t\boldsymbol{\mu}\), and
\(\boldsymbol{\theta} = \theta_1, \dots \theta_n\).

We can sample from the posterior of the Projected Normal distribution
using a Gibbs sampler \citep{chib1995understanding} with a
Metropolis-Hastings \citep{metropolis1953equation, hastings1970monte}
step, as done by \citet{Nunez-Antonio2005}, or by a slice sampling
\citep{hernandez2017general} step for \(r\). In \pkg{circbayes} this
second option was implemented in \proglang{C++} through \pkg{Rcpp}.

To obtain an MCMC sample from the Projected Normal posterior we can use
\code{pn_posterior}.

\begin{CodeChunk}

\begin{CodeInput}
R> pn_mod <- pn_posterior(th)
R> pn_mod 
\end{CodeInput}

\begin{CodeOutput}
     mean  mode    se  2.5% 97.5%
mu1 1.302 1.326 0.086 1.144 1.447
mu2 0.308 0.324 0.061 0.188 0.423
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{circbayes_RPackageForBayesianCircularStatistics_files/figure-latex/plot_vm-1} 

}

\caption[Three posterior fits of univariate symmetric circular probability densities, which are the von Mises distribution, the Projected Normal distribution, and the Power Batschelet distribution]{Three posterior fits of univariate symmetric circular probability densities, which are the von Mises distribution, the Projected Normal distribution, and the Power Batschelet distribution. Dashed lines give the 95 percent credible interval of the probability density.}\label{fig:plot_vm}
\end{figure}
\end{CodeChunk}

\hypertarget{alternatives}{%
\subsubsection{Alternatives}\label{alternatives}}

Two alternative approaches not used in \pkg{circbayes} are the wrapping
approach and the stereographic projection approach. The wrapping
approach augments the circular observations such that they can be
represented as \(\theta = x ~\text{mod} ~ 2\pi,\) where
\(x \in \mathbb{R}\) is some unobserved data point which has a known
distribution such as the Normal distribution. To reverse this, we can
say that \(x = \theta + k 2 \pi,\) where \(k \in \mathbb{Z}\) is a
latent wrapping factor. Methods for wrapped models are available in the
package \pkg{wrapped} \citep{nadarajah2017wrapped}. Another option is
given by the stereographic projection approach, which uses projection
from the real line to a circle below it \citep{abe2010symmetric}, but
few implementations or extensions exist.

\hypertarget{bayesian-circular-regression}{%
\section{Bayesian Circular
regression}\label{bayesian-circular-regression}}

\label{circreg}

Circular regression refers to regression models where the outcome is a
circular observation. If the circular observation \(\theta\) is a
predictor instead, often adding \(\cos\theta\) and \(\sin\theta\) to the
predictors suffices \citep{fisher1995statistical}.

For circular regression, \pkg{circbayes} implements two approaches which
we find the most promising. The first is a Generalized Linear Model
(GLM) with an arctangent link function and von Mises residuals. We will
call this the von Mises (VM) regression model, and it will be discussed
in Section \ref{vmreg}.

The second model is the Projected Normal (PN) regression model, where
the residuals have the Projected Normal distribution. While the
Projected Normal distribution is more difficult to work with, this leads
to a regression function that is simpler to work with. This model will
be discussed in Section \ref{projreg}.

Both models will exemplified using the \code{Motor} data
\citep{puglisi2017role} from \pkg{bpnreg}.

\hypertarget{circular-regression-with-von-mises-residuals}{%
\subsection{Circular regression with von Mises
residuals}\label{circular-regression-with-von-mises-residuals}}

\label{vmreg}

Consider a the problem of regressing a circular outcome \(\theta\), on a
column vector of continuous linear predictors
\(\boldsymbol{x}_i \in \mathbb{R}^K\), assumed to be standardized, and a
column vector of dichotomous predictors
\(\boldsymbol{d}_i \in \{0, 1\}^J\). Assume that each observed angle
\(\theta_i\) is generated independently from a von Mises distribution
\(\mathcal{M}(\theta_i \mid \mu_i, \kappa)\). Then, \(\mu_i\) is chosen
to be \begin{equation}
\mu_i = \beta_0 + \boldsymbol{\delta}^T \boldsymbol{d}_i + g(\boldsymbol{\beta}^T \boldsymbol{x}_i),
\end{equation} where \(\beta_0 \in [-\pi, \pi)\) is an offset parameter
which serves as a circular intercept,
\(\boldsymbol{\delta}\in [-\pi, \pi)^J\) is a column vector of circular
group difference parameters,
\(g(\cdot) : \mathbb{R} \rightarrow (-\pi, \pi)\) is a twice
differentiable link function, and \(\boldsymbol{\beta}\in \mathbb{R}^K\)
is a column vector of regression coefficients. The link function is
often chosen to be \(g(x) = 2 ~ \arctan(x),\) where \(\arctan(\cdot)\)
is the arctangent, the inverse tangent function.

This model was first described by \citet{fisher1992regression}, with an
initial application of a Bayesian version of this method given by
\citet{gill2010}. The version in \pkg{circbayes} given above, where
categorical predictors are treated separately, was developed in
\citet{mulder2017bayesian}.

The priors are assumed to be independent between
\(\{\beta_0, \kappa\},\) which has the conjugate prior for the
parameters of the von Mises distribution, and the predictors
\(\{\boldsymbol{\beta}, \boldsymbol{\delta}\},\) which are given a
Normal prior. As described by \citet{gill2010}, the likelihoods (or
posterior if a constant or very uninformative prior is used) of the
regression coefficients \(\{\boldsymbol{\beta}, \boldsymbol{\delta}\}\)
is ill-behaved in the sense that it can be multimodal and have non-zero
asymptotes. This issue can be solved by giving these parameters a
\(\text{Normal}(0, 1)\) prior (see \citet{mulder2017bayesian}), which is
only very weakly for this model if the predictors are standardized.
Therefore, this is the default.

The VM regression model can be performed using the function
\code{vm_reg}.

\begin{CodeChunk}

\begin{CodeInput}
R> Motor$AvAmp <- scale(Motor$AvAmp)
\end{CodeInput}
\end{CodeChunk}

\begin{CodeChunk}

\begin{CodeInput}
R> fit_vmreg <- vm_reg(Phaserad ~ AvAmp, data = Motor, 
R+                     prior_b0kp = c(0, 0, 0),
R+                     prior_btdt = c(0, 1))
R> fit_vmreg
\end{CodeInput}

\begin{CodeOutput}
Bayesian circular GLM 

Call:
vm_reg(formula = Phaserad ~ AvAmp, data = Motor, prior_b0kp = c(0, 
    0, 0), prior_btdt = c(0, 1))

MCMC run for 2000 its, 1000 used. 

Coefficients:
          estimate    se   2.5% 97.5%
Intercept    0.613 0.161  0.279 0.912
Kappa        1.540 0.368  0.761 2.178
AvAmp        0.060 0.082 -0.088 0.236

DIC:  125.831 
WAIC: 125.519 
\end{CodeOutput}
\end{CodeChunk}

The intercept \(\beta_0\) and concentration parameter \(\kappa\) are
sampled as described in Section \ref{vonmisespost}. The regression
coefficients \(\boldsymbol{\beta}\) and \(\boldsymbol{\delta}\) are
sampled using a Metropolis-Hastings step. For details, see
\citep{mulder2017bayesian}. This function will automatically select
dichotomous predictors and treat them accordingly. Posterior summaries
can then be obtained through \code{coef(fit_vmreg)}.

\hypertarget{circular-regression-with-projected-normal-residuals}{%
\subsection{Circular regression with Projected Normal
residuals}\label{circular-regression-with-projected-normal-residuals}}

\label{projreg}

In the Projected Normal (PN) regression model \citep{nunez2011bayesian},
the bivariate mean vector \(\mu\) is made dependent on a set of \(K\)
predictors \(\boldsymbol{X}\). This is done by defining the mean vector
in \eqref{eq:PNdistribution} as \begin{equation}
\boldsymbol{\mu}_{i} = \begin{pmatrix}
  \mu_{i}^{I}  \vspace{0.2cm}  \\
\mu_{i}^{II}
 \end{pmatrix}=\begin{pmatrix}
  (\boldsymbol{\beta}^{I})^{t}\boldsymbol{X}_{i}^{I}  \vspace{0.2cm}  \\
  (\boldsymbol{\beta}^{II})^{t}\boldsymbol{X}_{i}^{II} 
 \end{pmatrix},
\end{equation} where \(\boldsymbol{\beta}^{I} \in \mathbb{R}^K\) and
\(\boldsymbol{\beta}^{II} \in \mathbb{R}^K\) are vectors with regression
coefficients and \(\boldsymbol{X}_{i}^{I}\) and
\(\boldsymbol{X}_{i}^{II}\) are design matrices, which are often equal
but may also be different. As in the VM regression model, the mean
direction is dependent on the predictors through a link function.

In \pkg{circbayes} PN regression is implemented through the function
\code{pn_reg()}. We can fit the PN regression model as follows.

\begin{CodeChunk}

\begin{CodeInput}
R> fit_pnreg <- pn_reg(Phaserad ~ AvAmp, data = Motor)
R> coef(fit_pnreg)$linear
\end{CodeInput}

\begin{CodeOutput}
                   mean        mode        se       2.5%     97.5%
(Intercept)  0.97189043  0.98983375 0.1931467  0.5982540 1.3378849
AvAmp       -0.03403904 -0.08624685 0.1980534 -0.3994491 0.3765813
(Intercept)  0.65891290  0.65682925 0.1757234  0.3266365 0.9975990
AvAmp        0.10443282  0.10773670 0.1832027 -0.2368230 0.4708930
\end{CodeOutput}
\end{CodeChunk}

Bayesian inference is performed using a Gibbs sampler with slice
sampling step for \(r\) \citep{hernandez2017general} and implemented in
\proglang{C++} through \pkg{Rcpp}.

\hypertarget{regression-coefficients}{%
\subsubsection{Regression Coefficients}\label{regression-coefficients}}

The vectors \(\boldsymbol{\beta}^{I}\) and \(\boldsymbol{\beta}^{II}\)
contain the linear regression coefficients. MCMC samples can be obtained
using \code{posterior_samples()}.

Posterior summaries can be obtained through \code{coef_lin(fit_pnreg)}.

Note that due to the parametrization in bivariate space there are two
regression coefficients for each predictor. In order to facilitate
interpretation, three circular regression coefficients were introduced
in \citet{CremersMulderKlugkist2017}. These coefficients in circular
space are obtained by a reparametrization of the marginal effect of one
predictor \(x\) by \begin{align}
  \label{reparametrization_ch3}
  \hat{\theta} = \text{atan2}\left(\hat{y}^{II}, \:\hat{y}^{I}\right) &= \text{atan2}\left(\beta_{0}^{II} + \beta_{1}^{II} x, \: \beta_{0}^{I} + \beta_{1}^{I} x\right) \nonumber\\
  &= a_c + \arctan\left[b_{c}\left(x - a_{x}\right)\right],
\end{align} where \(\text{atan2}\) is the link function,
\(\hat{\theta}\) is the circular predicted value, \(\hat{y}^{II}\) and
\(\hat{y}^{I}\) are the predicted values in bivariate space and \(b_c\)
is the circular regression coefficient. Note that the two other
regression coefficients \(SAM\) and \(AS\) are dependent on both \(b_c\)
and \(x\) and return the slope at different parts of the circular
regression line. Posterior summaries of the circular regression
coefficients can be obtained through \code{coef_circ(fit_pnreg)}. For
categorical predictors, the posterior descriptives of the predicted
circular values of the different (combinations of) categories are
returned.

\begin{CodeChunk}
\begin{figure}

{\centering \includegraphics{circbayes_RPackageForBayesianCircularStatistics_files/figure-latex/plot_reg-1} 

}

\caption[Univariate regression plots for the von Mises and Projected Normal regression models]{Univariate regression plots for the von Mises and Projected Normal regression models. The dashed lines provide the 95 percent credible interval of the regression line.}\label{fig:plot_reg}
\end{figure}
\end{CodeChunk}

\hypertarget{comparison-between-the-two-regression-models}{%
\subsection{Comparison between the two regression
models}\label{comparison-between-the-two-regression-models}}

Although the structure of two the models is quite different, both the VM
and PN regression models predict a point on the circle conditional on a
set of predictors, and both have a fairly simple residual structure. In
fact, it can also be seen that both methods feature a regression
function that takes the shape of an arctangent function. Therefore, one
might wonder how to select a regression model for any practical problem.
We will highlight three differences.

A first major distinction between the two models is that homogeneity of
variance is enforced for the VM regression model, while the PN
regression model has an inherent heterogeneity of variance, so that the
circular residual variance changes as the predicted value changes.
Whether heterogeneity of variance is required depends on whether the
data exhibits this property. In order to test this, one may perform
model comparison after running both models.

A second major distinction is that for the PN regression model, a single
predictor is able to move the circular outcome across a semicircle,
while in the VM regression model the reach of a single predictor can be
selected, but is usually the full circle.

Thirdly, the PN regression model admits extensions in a somewhat more
straightforward manner, because existing solutions for bivariate data
can simply be plugged into the model. This has for instance led to a
hierarchical version of the PN regression model, which will be presented
next.

To make the selection between the two regression models, we recommend
applying both methods to the dataset and applying the methods discussed
in Section \ref{hyptest} to make a selection of the best-fitting method.

\hypertarget{hierarchical-projected-normal-regression}{%
\subsection{Hierarchical Projected Normal
regression}\label{hierarchical-projected-normal-regression}}

The Hierarchical Projected Normal regression model
\citep{nunez2014bayesian} is useful for modeling dependent circular
data. We define the mean vector in \eqref{eq:PNdistribution} as
\begin{equation}
\boldsymbol{\mu}_{ij} = \begin{pmatrix}
  \mu_{ij}^{I}  \vspace{0.2cm}  \\
\mu_{ij}^{II}
 \end{pmatrix}=\begin{pmatrix}
  (\boldsymbol{\beta}^{I})^{t}\boldsymbol{X}_{ij}^{I} + (\boldsymbol{b}_i^{I})^t\boldsymbol{Z}_{ij}^{I} \vspace{0.2cm}  \\
  (\boldsymbol{\beta}^{II})^{t}\boldsymbol{X}_{ij}^{II} + (\boldsymbol{b}_i^{II})^t\boldsymbol{Z}_{ij}^{II}
 \end{pmatrix},
\end{equation} where \(\boldsymbol{\beta}^{I}\) and
\(\boldsymbol{\beta}^{II}\) are vectors with fixed effect coefficients
and intercept, \(\boldsymbol{b}_{i}^{I}\) and
\(\boldsymbol{b}_{i}^{II}\) are vectors with random effects for each
individual and \(\boldsymbol{X}_{ij}^{I}\),
\(\boldsymbol{X}_{ij}^{II}\), \(\boldsymbol{Z}_{ij}^{I}\) and
\(\boldsymbol{Z}_{ij}^{II}\) are design matrices.

In \pkg{circbayes} hierarchical PN regression is implemented through the
function \code{pn_me_reg()}. Using the \code{Maps} data
\citep{Warren2017} we can fit a hierarchical PN regression model as
follows:

\begin{CodeChunk}

\begin{CodeInput}
R> pmer_mod <- pn_me_reg(formula = Error.rad ~ Maze + 
R+                         Trial.type + L.c + (1 | Subject), 
R+                       data = Maps)
R> coef(pmer_mod)$linear
\end{CodeInput}

\begin{CodeOutput}
                    mean        mode         se        2.5%       97.5%
(Intercept)  2.359133810  2.27095206 0.25686138  1.89596834  2.84644389
Maze1       -0.903994114 -0.92657074 0.30005444 -1.46256760 -0.29205615
Trial.type1  0.384382370  0.39535677 0.26053229 -0.11523313  0.91123628
L.c          0.002273315  0.01092830 0.04034233 -0.08823473  0.07168172
(Intercept) -0.564580713 -0.54251394 0.16514339 -0.86298615 -0.21642491
Maze1        0.507874981  0.35205189 0.21767993  0.07615177  0.93334560
Trial.type1  1.140930788  1.17578280 0.18339447  0.81166425  1.50050353
L.c         -0.007023267 -0.01905147 0.03232604 -0.07046604  0.05272091
\end{CodeOutput}
\end{CodeChunk}

Bayesian estimation is performed using a Gibbs sampler with slice
sampling step for \(r\) \citep{hernandez2017general}.

\hypertarget{regression-coefficients-1}{%
\subsubsection{Regression
Coefficients}\label{regression-coefficients-1}}

MCMC samples for the fixed and random effects are given by
\code{posterior_samples(pmer_mod)}, posterior summaries can be obtained
through \code{coef_lin(pmer_mod)} and \code{coef_ran(pmer_mod)}. To
obtain posterior summaries for the circular coefficients
\citep{longitudinalpaper} we use \code{coef_circ(pmer_mod)} and
\code{coef_ran(pmer_mod, type = `circular')}.

\hypertarget{density-estimation-problems}{%
\section{Density estimation
problems}\label{density-estimation-problems}}

\label{densest}

A different class of circular data problems deals with estimating
densities on the circle, where the goal is to predict or understand
where on the circle the circular observations are likely to occur. This
is particularly common for multimodal circular observations, especially
derived from temporal measurements, such modeling events spread across
the hours of the day or the days of the year. For example, such models
have been succesfully applied in predicting wildfires
\citep{ameijeiras2018directional, ley2018applied} which exhibit a strong
seasonal pattern over the year. Another example is the prediction of
crime times over the day \citep{ashby2013comparison}. Outside the
temporal domain, circular mixtures have been applied to eye movement
directions \citep{van2016infants}.

The simplest approach is to obtain the posterior for a single density.
For this, methods are provided for von Mises, Inverse Batschelet and
Projected Normal distributions. All three require MCMC sampling and were
described in Section \ref{sec:probdist}. However, often the data under
consideration is multimodal or otherwise irregular. For such data,
\pkg{circbayes} implements two models which can fit a wide variety of
circular data distributions.

The first is a family of mixture distributions, containing both von
Mises and Batschelet Mixtures. Mixture models are appropriate if we
expect the circular observations to be generated by multiple distinct
data generating processes, for which labels were not observed. In these
cases, interest is in parameter estimation, posterior prediction of new
observations, and possibly reassigning the labels of the unobserved
classes. A disadvantage is that the number of underlying components must
be chosen. These models are discussed in Section \ref{mixmod}.

The second is the Dirichlet process mixture (DPM) model. This is a model
from the field of Bayesian non-parametrics, where the number of
components does not need to be set, while the model can still learn any
data generating density. The DPM model is useful when one does not care
about or know about the number of components required, but rather wishes
to flexibly estimate possible probability densities on the circle. This
model is discussed in Section \ref{sec:dpmjss}.

These models will be exemplified on the turtles data set
\citep{stephens1969techniques, fisher1995statistical}, provided in
\pkg{circular} as \code{circular::fisherB3c}.

\hypertarget{mixture-models}{%
\subsection{Mixture models}\label{mixture-models}}

\label{mixmod}

The mixture models available in \pkg{circbayes} are of the form
\begin{equation}
 f(\theta \mid \boldsymbol{\mu}, \boldsymbol{\kappa}, \boldsymbol{\lambda}, \boldsymbol{\alpha}) = \sum_{j = 1}^J\alpha_j f_B(\theta \mid \mu_j, \kappa_j, \lambda_j),
\end{equation} where \(j\) indexes the \(J\) components in the mixture,
\(\alpha_j\) are component weights, and \(f_B(\cdot)\) is the chosen
probability density for each component in the mixture. In this case, the
probability density can be either the von Mises distribution (in which
case we fix \(\boldsymbol{\lambda}= \boldsymbol{0}_J,\) a zero vector)
or the Inverse Batschelet distribution.

The von Mises mixture can be performed by

\begin{CodeChunk}

\begin{CodeInput}
R> th <- circular::fisherB3c
R> vm_mix_fit <- vm_mix(th = th, n_comp = 2)
R> vm_mix_fit
\end{CodeInput}

\begin{CodeOutput}
Mixture of power Batschelet distributions, using method 'bayes'.
              mu       kp lam      alph  circ_var   circ_sd
comp_1 -2.079323 9.321815   0 0.1719184 0.0552646 0.3371954
comp_2  1.107344 2.670828   0 0.8280816 0.2176319 0.7006139
\end{CodeOutput}
\end{CodeChunk}

Mixtures of Batschelet distributions, either Power or Inverse, can be
performed the \code{bat_mix} command.

\begin{CodeChunk}

\begin{CodeInput}
R> pb_mix_fit <- bat_mix(th = th, n_comp = 2, bat_type = "power")
R> pb_mix_fit
\end{CodeInput}

\begin{CodeOutput}
             mean median     se   2.5%  97.5%
mu_1       -2.069 -2.069  0.104 -2.277 -1.883
mu_2        1.103  1.105  0.067  0.974  1.237
kp_1       22.491 16.042 20.898  3.153 91.411
kp_2        2.232  2.268  0.634  1.308  3.601
lam_1      -0.062 -0.073  0.165 -0.355  0.224
lam_2       0.275  0.148  0.308 -0.157  0.819
alph_1      0.150  0.149  0.053  0.052  0.260
alph_2      0.850  0.851  0.053  0.740  0.948
circ_var_1  0.053  0.043  0.050  0.004  0.197
circ_var_2  0.277  0.244  0.089  0.161  0.467
circ_sd_1   0.306  0.298  0.139  0.091  0.662
circ_sd_2   0.802  0.749  0.153  0.592  1.121
\end{CodeOutput}
\end{CodeChunk}

For this type of mixture, any parameter can be fixed to specific values
by providing an \(4 \times J\) matrix of parameter values to the
argument \code{fixed_pmat}, where any value to be freely estimated is
set to \code{NA}.

\hypertarget{bayesian-nonparametrics-for-circular-data}{%
\subsection{Bayesian Nonparametrics for Circular
data}\label{bayesian-nonparametrics-for-circular-data}}

\label{sec:dpmjss}

The Dirichlet Process Mixture model is a popular method in Bayesian
non-parametrics due to its ability to fit data from any data generating
process. It essentially is an infinite mixture model, where the
effective number of mixtures is regulated by a stick-breaking prior
\citep{ishwaran2001gibbs}. Therefore, it can be seen as a mixture model
for which we do not need to select a fixed number of components. Another
way to view these methods is as a Bayesian alternative to kernel density
methods such as those in \pkg{NPCirc} \citep{JSSv061i09}, although the
DPM model is a true statistical model rather than a descriptive method.
For a more in-depth introduction, see
\citet[ch. 23]{gelman2003bayesian}.

We provide methods for circular distributions that can be used with the
\pkg{dirichletprocess} package \citep{dirichletprocesspackage}. For an
in-depth introduction to the DPM model and the use of the
\pkg{dirichletprocess} package, see its vignette. The Von Mises
Dirichlet process model can be written as \begin{align}
\theta_i \mid (\mu_i, \kappa_i) &\sim \mathcal{M}(\mu_i, \kappa_i) \\
(\mu_i, \kappa_i) \mid G  &\sim G \\
G &\sim \text{DP}(P_0, \alpha),
\end{align} where \(P_0\) is a base distribution (or base measure) on
the parameters of the von Mises distribution, and \(\alpha\) is a
parameter of the Dirichlet Process which influences the number of
mixture components used. We use the conjugate prior given in Equation
\ref{eqn:vmconjprior} as the base distribution. In contrast to earlier
applications of this prior, it must be proper here (that is, integrate
to 1). It is generally sufficient to set \(c \geq 1,\) although note
that this means that if we also set \(R_0 > 0,\) the prior is
informative towards the mean direction \(\mu_0\). Therefore, the default
sets \(\{\mu_0 = 0, R_0 = 0, c = 1\}.\)

The parameter \(\alpha\) is given a \(\text{Gamma}(a, b)\) prior and
sampled in the MCMC algorithm to avoid the need to tune this parameter.
Setting the parameters of this prior \(\{a, b\}\) must still be done
with care, however.

This Dirichlet Process Mixture model can be run by

\begin{CodeChunk}

\begin{CodeInput}
R> dpm_model <- vm_dpm(th = th, alphaPriors = c(2, 4), g0Priors = c(0, 0, 1))
R> dpm_model
\end{CodeInput}

\begin{CodeOutput}
Dirichlet process object run for 1000 iterations.
                                    
  Mixing distribution       vonmises
  Base measure parameters    0, 0, 1
  Alpha Prior parameters        2, 4
  Conjugacy                conjugate
  Sample size                     76
                                    
  Mean number of clusters       3.63
  Median alpha                  0.51
\end{CodeOutput}
\end{CodeChunk}

In each iteration of the MCMC algorithm of the DPM model, the number of
parameters changes. Therefore, the posterior samples are provided as a
list.

\begin{CodeChunk}
\begin{figure}

{\centering \includegraphics{circbayes_RPackageForBayesianCircularStatistics_files/figure-latex/plot_mix-1} 

}

\caption[Density estimation plots, for the von Mises mixture, Batschelet mixture, and Dirichlet Process Mixture Model]{Density estimation plots, for the von Mises mixture, Batschelet mixture, and Dirichlet Process Mixture Model. Dashed lines indicate the 95 percent credible interval of the probability density.}\label{fig:plot_mix}
\end{figure}
\end{CodeChunk}

\hypertarget{hypothesis-tests-and-model-comparison}{%
\section{Hypothesis tests and model
comparison}\label{hypothesis-tests-and-model-comparison}}

\label{hyptest}

Circular statistics packages such as \pkg{circular} often include a
large variety of frequentist hypothesis tests. Bayesian counterparts of
almost all of such tests are available in \pkg{circbayes}. A Bayesian
approach to testing two discrete hypotheses against one another is to
update the prior odds of two hypotheses by multiplying them by the Bayes
factor \citep{kass1995bayes, jeffreys1961theory}, in order to produce
the posterior odds of the two hypotheses. It should be noted that
\textit{hypotheses} here refers to distinct statistical models from
which the data may have been generated, and we will therefore refer to
them as model \(M_s\) with an associated set of parameters
\(\boldsymbol{\phi}_s\). In fact, the Bayes factor is computed as the
ratio of the marginal likelihoods of the two models \begin{equation}
BF_{12} = \frac{p(D \mid M_1)}{p(D \mid M_2)},
\end{equation} where \(D\) represents the data, and the marginal
likelihood is given by \begin{equation} \label{eqn:marglik}
p(D \mid M_s) = \int p(D, \boldsymbol{\phi}_s \mid H_s) d \boldsymbol{\phi}_s,
\end{equation} where the integral is over the parameter space of
\(\boldsymbol{\phi}_s.\)

An advantage of the Bayesian Hypothesis test is that it is possible to
find evidence in favor of either hypothesis, with either possible to be
chosen as most likely. This is in stark contrast with the frequentist
Null Hypothesis Significance Testing (NHST) approach, where the null
hypothesis can at most attain the status of `not rejected'.

A disadvantage of the Bayesian hypothesis test is that care must be
taken towards the influence of the prior. Improper priors, or priors
chosen unwisely, can result in untrustworthy results. Therefore, we
generally recommend to employ the Bayesian hypothesis test only with
proper and at least weakly informative priors.

Besides the posterior odds between two hypotheses, we can also obtain
the posterior model probability of each model within a set of models
\(M_1, \dots, M_Q\), by computing \begin{equation}
p(M_s \mid D) = \frac{p(D \mid M_s)}{\sum_{q = 1}^Q p(D \mid M_q)}.
\end{equation} This is an intuitive probability of interest, because it
represents our current belief that \(M_s\) is true rather than one of
the other models under consideration. This probability might further be
used to make decisions, or in Bayesian model averaging
\citep{hoeting1999bayesian, bao2010bias}.

Due to the quite general form of the Bayesian hypothesis test, any two
statistical models can be compared as long as their marginal likelihoods
can be computed. Section \ref{infcrit} will first provide a simple
alternative form of model comparison, which is to compare models on
their (Bayesian) information criteria \citep[Ch. 7]{gelman2003bayesian}
instead of computing Bayes Factors. If nested models are compared, Bayes
factors can be obtained somewhat easily, as discussed in Section
\ref{sec:nested}. A more general solution using bridge sampling, which
works for all models, is discussed in Section \ref{sec:bridge}.

\hypertarget{information-criteria}{%
\subsection{Information criteria}\label{information-criteria}}

\label{infcrit}

For all models in \pkg{circbayes}, information criteria can be obtained
by running \code{inf_crit()} on the model.

\begin{CodeChunk}

\begin{CodeInput}
R> inf_crit(fit_vmreg)
\end{CodeInput}

\begin{CodeOutput}
               value
AIC_Bayes 125.097302
p_DIC       3.366769
p_DIC_alt   3.744478
DIC       125.830840
DIC_alt   126.586258
p_WAIC1     3.055156
p_WAIC2     3.240075
WAIC1     125.519227
WAIC2     125.889066
\end{CodeOutput}
\end{CodeChunk}

For most models, we provide the DIC \citep{spiegelhalter2002bayesian}
and the WAIC \citep{watanabe2010asymptotic}. For a discussion of the
strengths and weaknesses of these, see
\citet[Ch. 7]{gelman2003bayesian}. Model comparison on the basis of
these information criteria can be seen as an approximation to model
comparison using the marginal likelihood, so they should only be
preferred if computing the marginal likelihood is difficult or
impossible.

\hypertarget{nested-comparison-bayes-factors}{%
\subsection{Nested comparison Bayes
Factors}\label{nested-comparison-bayes-factors}}

\label{sec:nested}

Sometimes interest is in comparing a model \(M_s\) with a nested
submodel \(M^\ast_s,\) which only differs in that some parameters are
set to a fixed value. For example, one may compare a regression model
with regression coefficient \(\beta\) with a regression model where that
\(\beta = 0.\) For those cases, the Savage-Dickey method of obtaining
Bayes factors is applicable
\citep{dickey1970weighted, o2004kendall, wagenmakers2010sdd} and
markedly easier to compute than the marginal likelihood. Similarly, if
one wishes to compare hypotheses related to inequalities, such as
\(\beta < 0\) versus \(\beta > 0\), then the encompassing prior approach
is applicable \citet{klugkist2005inequality, wetzels2010encompassing}.
Bayes factors for both approaches are provided for the VM and PN
regression model.

\hypertarget{marginal-likelihood-computation}{%
\subsection{Marginal Likelihood
computation}\label{marginal-likelihood-computation}}

\label{sec:bridge}

To compute the marginal likelihood in Equation \ref{eqn:marglik} we must
integrate once over each parameter of the model. Therefore, models with
more than a few parameters tend to have a marginal likelihood that is
difficult to compute, because numerical integration is no longer an
option. For those models, there exists a literature of methods to
approximate the marginal likelihood (for an overview, see
\citet{friel2012estimating} and \citet{ardia2012comparative}).

We employ bridge sampling
\citep{meng1996simulating, gronau2017tutorial}, which is implementend in
the package \pkg{bridgesampling} \citep{gronau2017bridgesampling} to
which methods for circular variables were contributed. An overview of
the methods used for computing Bayesian hypothesis tests for each of the
models in \pkg{circbayes} is given in Table \ref{tab:models}. For almost
all of the models, computing the marginal likelihood through bridge
sampling is possible.

This most general form of the Bayesian hypothesis test thus allows us to
compare any two models, which do not have to be nested. For example,
this approach can be used to test whether the VM or PN regression model
fits the data better, which set of predictors perform well in a
regression, or whether a mixture model requires three or four
components.

\hypertarget{hypothesis-testing-examples}{%
\subsection{Hypothesis testing
examples}\label{hypothesis-testing-examples}}

This section will provide some examples of hypothesis tests that could
be performed in the framework described previously.

\hypertarget{testing-for-circular-uniformity}{%
\subsubsection{Testing for circular
uniformity}\label{testing-for-circular-uniformity}}

An important initial test is whether the data is uniform. Specific
frequentists tests are most powerful against a specific alternative.
Similarly, Bayesian hypothesis tests require selection of a specific
alternative. The equivalent of the Rayleigh test
\citep{mardia2009directional, brazier1994confidence} in the Bayesian
paradigm can be performed by comparing the von Mises marginal likelihood
with the circular uniform marginal likelihood.

\begin{CodeChunk}

\begin{CodeInput}
R> th <- circular::fisherB9c
R> 
R> vm_mod <- vm_posterior(th, prior = c(0, 0, 1))
R> 
R> cu_ml  <- marg_lik_circ_unif(th)
R> vm_ml  <- marg_lik(vm_mod)
R> 
R> bht_compare(uniform = cu_ml, von_mises = vm_ml)
\end{CodeInput}

\begin{CodeOutput}
Bayesian Hypothesis Test
    Comparing 2 models: uniform, von_mises

[Log Marginal Likelihood]

  uniform von_mises 
-512.7677 -513.9991 


[Posterior Model Probabilities]

  uniform von_mises 
 0.774065  0.225935 


[Pairwise log Bayes Factors]
  (Positive values support models in the rows)

               Versus: 
Support for:      uniform von_mises
      uniform    0.000000  1.231408
      von_mises -1.231408  0.000000
\end{CodeOutput}
\end{CodeChunk}

For this dataset, the uniform distribution is slightly preferred,
although the Bayes factor, which is 3.43, is still somewhat undecided.

\hypertarget{goodness-of-fit}{%
\subsubsection{Goodness-of-fit}\label{goodness-of-fit}}

Comparison of the goodness of fit of several models is common in
circular statistics (for example, in \citet{pewsey2013circular}), and
allows us to select the correct probability density for a certain set of
data. We apply this to the classic red ant dataset
\citep{jander1957optische, fisher1995statistical}, which is available in
\pkg{circular} as \code{circular::fisherB7c}.

\begin{CodeChunk}

\begin{CodeInput}
R> ants <- circular::fisherB7c
R> 
R> vm_ants <- vm_posterior(ants)
R> pb_ants <- bat_posterior(ants, bat_type = "power")
R> pn_ants <- pn_posterior(ants)
R> 
R> bht_compare(von_mises = marg_lik(vm_ants), 
R+             powerbat  = marg_lik(pb_ants), 
R+             projnorm  = marg_lik(pn_ants))
\end{CodeInput}

\begin{CodeOutput}
Bayesian Hypothesis Test
    Comparing 3 models: von_mises, powerbat, projnorm

[Log Marginal Likelihood]

von_mises  powerbat  projnorm 
-144.1331 -135.4386 -220.1066 


[Posterior Model Probabilities]

   von_mises     powerbat     projnorm 
1.674689e-04 9.998325e-01 1.694645e-37 


[Pairwise log Bayes Factors]
  (Positive values support models in the rows)

               Versus: 
Support for:     von_mises   powerbat projnorm
      von_mises   0.000000  -8.694545 75.97346
      powerbat    8.694545   0.000000 84.66801
      projnorm  -75.973462 -84.668008  0.00000
\end{CodeOutput}
\end{CodeChunk}

Here, we can conclude that due to the peaked shape of the data, the
Batschelet distribution fits the best.

\hypertarget{multi-sample-anova-type-test}{%
\subsubsection{Multi-sample (ANOVA-type)
test}\label{multi-sample-anova-type-test}}

Often, the question of interest is whether multiple samples have the
same or different distributions. We show this on the \code{pigeons} data
\citep{gagliardo2008navigational, fisher1995statistical} from
\pkg{circular}. The question is whether bearings of pigeons differ
dependent on a treatment condition. One model under consideration has
all three groups with equal distribution. Two others have the three
groups with equal concentration, but different means (called homogeneous
in variance), while a third has all three groups different in both mean
and variance (called heterogeneous in variance). The homogeneous model
must be run in a regression context, while the heterogeneous model just
consists of separate runs of the von Mises posterior. Instead of
choosing a specific alternative, we investigate all three models at the
same time and compare.

\begin{CodeChunk}

\begin{CodeInput}
R> pigeons <- circular::pigeons
R> pigeons$bearing <- circular::circular(pigeons$bearing, units = "degrees")
R> pig_dummy <- cbind(bearing = pigeons$bearing, 
R+                    model.matrix(bearing ~ treatment, pigeons)[, -1])
R> 
R> nodiff_pigeons <- vm_posterior(pigeons$bearing)
R> hom_pigeons    <- vm_reg(bearing ~ treatmenton + treatmentv1, data = pig_dummy)
R> het_pigeonsc   <- vm_posterior(pigeons[pigeons$treatment == "c", 'bearing'])
R> het_pigeonsv1  <- vm_posterior(pigeons[pigeons$treatment == "v1", 'bearing'])
R> het_pigeonson  <- vm_posterior(pigeons[pigeons$treatment == "on", 'bearing'])
R> 
R> 
R> bht_compare(nodiff     = marg_lik(nodiff_pigeons), 
R+             homogen    = marg_lik(hom_pigeons), 
R+             heterogen  = sum(marg_lik(het_pigeonsc), 
R+                              marg_lik(het_pigeonsv1), 
R+                              marg_lik(het_pigeonson)))
\end{CodeInput}

\begin{CodeOutput}
Bayesian Hypothesis Test
    Comparing 3 models: nodiff, homogen, heterogen

[Log Marginal Likelihood]

   nodiff   homogen heterogen 
-161.5862 -235.0694 -145.8102 


[Posterior Model Probabilities]

      nodiff      homogen    heterogen 
1.407859e-07 1.718845e-39 9.999999e-01 


[Pairwise log Bayes Factors]
  (Positive values support models in the rows)

               Versus: 
Support for:       nodiff  homogen heterogen
      nodiff      0.00000 73.48314 -15.77603
      homogen   -73.48314  0.00000 -89.25917
      heterogen  15.77603 89.25917   0.00000
\end{CodeOutput}
\end{CodeChunk}

From this test, we can conclude that the heterogeneous model is strongly
preferred. Therefore, we can conclude that the treatment influences both
the mean direction and the circular variance.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

The \proglang{R} package \pkg{circbayes} was introduced which performs
Bayesian inference for a wide variety of circular data models. The
methods introduced include regression models, mixture models,
non-parametric models, and hypothesis testing between the models.

Because the package provides a single front-end for Bayesian versions of
many of the most popular circular data methods, we hope users are able
to easily try out several models and compare them, to find the one that
fits the data best. Particularly, the

In addition, providing easy access to Bayesian methods for circular data
might motivate users to try out Bayesian approaches in situations where
they may provide modeling advantages. Such situations may range form
employing informative priors, performing goodness-of-fit tests between
several competing models, or performing hierarchical regression models.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

This work was supported by a ------ grant awarded to ------ from -----
(------).


\hypertarget{acknowledgements}{%
	\section{Acknowledgements}\label{acknowledgements}}

This work was supported by a ------ grant awarded to ------ from -----
(------).

\bibliography{C:/Dropbox/LiteratureCircular/CircularData}



\end{document}



